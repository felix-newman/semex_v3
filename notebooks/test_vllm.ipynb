{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 23:22:27 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from vllm import LLM\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from loguru import logger\n",
    "\n",
    "# Set up logging\n",
    "logger.remove()\n",
    "logger.add(lambda msg: print(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path: str | Path) -> str:\n",
    "    \"\"\"Encode image to base64 string.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "def create_vl_prompt(image_path: str | Path, text: str) -> list:\n",
    "    \"\"\"Create a vision-language prompt with image and text.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": encode_image(image_path)\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-25 23:24:25.430 | INFO     | __main__:<module>:7 - Initializing VLLM...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a2dd3d1b9a4ec19294dbdfcb40362e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdbbdcf481142c9ae997ab1f522c2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 23:24:34 [config.py:585] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-25 23:24:34 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d4e235fddf470eac89a495de76d658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716017c420bb4227a8dd01de368d130c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573131eccc2244f7be1f47f861c7b198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa852c479aa14b19b4edd61a0864f80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e2f09a0269405ab0ea5ac3f56d6ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 23:24:40 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='Qwen/Qwen2.5-VL-3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-VL-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-VL-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 03-25 23:24:41 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7e81b882bcd0>\n",
      "INFO 03-25 23:24:41 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-25 23:24:41 [cuda.py:220] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 23:24:42 [gpu_model_runner.py:1174] Starting to load model Qwen/Qwen2.5-VL-3B-Instruct...\n",
      "WARNING 03-25 23:24:42 [vision.py:97] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "INFO 03-25 23:24:42 [config.py:3243] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]\n",
      "WARNING 03-25 23:24:42 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 03-25 23:24:43 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde90420e60a4707b7472edac6fac75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e678451a91e44f928128cbab1cc32df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 23:29:48 [weight_utils.py:281] Time spent downloading weights for Qwen/Qwen2.5-VL-3B-Instruct: 305.503288 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add76a76669c48eabf6baae9b9be21fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83c1d3f8ea4480daea7b3022745b7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 23:29:50 [loader.py:447] Loading weights took 1.08 seconds\n",
      "INFO 03-25 23:29:50 [gpu_model_runner.py:1186] Model loading took 7.1557 GB and 307.809956 seconds\n",
      "INFO 03-25 23:29:50 [gpu_model_runner.py:1456] Encoder cache will be initialized with a budget of 98304 tokens, and profiled with 1 video items of the maximum feature size.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f7dcc061e04619976e4c0ff5ee209f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 03-25 23:29:55 [core.py:343] EngineCore hit an exception: Traceback (most recent call last):\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 335, in run_engine_core\n",
      "ERROR 03-25 23:29:55 [core.py:343]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 03-25 23:29:55 [core.py:343]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 290, in __init__\n",
      "ERROR 03-25 23:29:55 [core.py:343]     super().__init__(vllm_config, executor_class, log_stats)\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 63, in __init__\n",
      "ERROR 03-25 23:29:55 [core.py:343]     num_gpu_blocks, num_cpu_blocks = self._initialize_kv_caches(\n",
      "ERROR 03-25 23:29:55 [core.py:343]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 122, in _initialize_kv_caches\n",
      "ERROR 03-25 23:29:55 [core.py:343]     available_gpu_memory = self.model_executor.determine_available_memory()\n",
      "ERROR 03-25 23:29:55 [core.py:343]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 66, in determine_available_memory\n",
      "ERROR 03-25 23:29:55 [core.py:343]     output = self.collective_rpc(\"determine_available_memory\")\n",
      "ERROR 03-25 23:29:55 [core.py:343]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 03-25 23:29:55 [core.py:343]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 03-25 23:29:55 [core.py:343]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/utils.py\", line 2255, in run_method\n",
      "ERROR 03-25 23:29:55 [core.py:343]     return func(*args, **kwargs)\n",
      "ERROR 03-25 23:29:55 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "ERROR 03-25 23:29:55 [core.py:343]     return func(*args, **kwargs)\n",
      "ERROR 03-25 23:29:55 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 157, in determine_available_memory\n",
      "ERROR 03-25 23:29:55 [core.py:343]     self.model_runner.profile_run()\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1483, in profile_run\n",
      "ERROR 03-25 23:29:55 [core.py:343]     batched_dummy_mm_inputs = MultiModalKwargs.as_kwargs(\n",
      "ERROR 03-25 23:29:55 [core.py:343]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/multimodal/inputs.py\", line 661, in as_kwargs\n",
      "ERROR 03-25 23:29:55 [core.py:343]     json_mapped = json_map_leaves(\n",
      "ERROR 03-25 23:29:55 [core.py:343]                   ^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/jsontree.py\", line 33, in json_map_leaves\n",
      "ERROR 03-25 23:29:55 [core.py:343]     return {k: json_map_leaves(func, v) for k, v in value.items()}\n",
      "ERROR 03-25 23:29:55 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/jsontree.py\", line 33, in <dictcomp>\n",
      "ERROR 03-25 23:29:55 [core.py:343]     return {k: json_map_leaves(func, v) for k, v in value.items()}\n",
      "ERROR 03-25 23:29:55 [core.py:343]                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/jsontree.py\", line 39, in json_map_leaves\n",
      "ERROR 03-25 23:29:55 [core.py:343]     return func(value)\n",
      "ERROR 03-25 23:29:55 [core.py:343]            ^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343]   File \"/home/felix/repos/felix-newman/projects/semex_v3/.venv/lib/python3.11/site-packages/vllm/multimodal/inputs.py\", line 662, in <lambda>\n",
      "ERROR 03-25 23:29:55 [core.py:343]     lambda x: x.to(device, non_blocking=True),\n",
      "ERROR 03-25 23:29:55 [core.py:343]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 03-25 23:29:55 [core.py:343] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 165.12 MiB is free. Including non-PyTorch memory, this process has 7.47 GiB memory in use. Of the allocated memory 7.22 GiB is allocated by PyTorch, and 156.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "ERROR 03-25 23:29:55 [core.py:343] \n",
      "CRITICAL 03-25 23:29:55 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up logging\n",
    "logger.remove()\n",
    "logger.add(lambda msg: print(msg))\n",
    "\n",
    "# Initialize VLLM\n",
    "model_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "logger.info(\"Initializing VLLM...\")\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"bfloat16\",\n",
    "    tensor_parallel_size=1  # Adjust based on your GPU count\n",
    ")\n",
    "logger.info(\"VLLM initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test with an image\n",
    "image_path = \"../data/1.jpg\"  # Adjust path to your test image\n",
    "text = \"Describe this image in detail.\"\n",
    "\n",
    "prompt = create_vl_prompt(image_path, text)\n",
    "\n",
    "# Generate response\n",
    "outputs = llm.generate([prompt], temperature=0.7, max_tokens=512)\n",
    "\n",
    "# Print response\n",
    "for output in outputs:\n",
    "    print(\"\\nResponse:\")\n",
    "    print(output.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
